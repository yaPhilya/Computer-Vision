{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feature Interpolation\n",
    "\n",
    "## Описание задания\n",
    "\n",
    "В этом задании вы ознакомитесь с довольно простым, но очень интересным подходом для семантической модификации (морфирования) изображений. От вас потребуется реализовать основные компоненты этого алгоритма с использованием библиотеки TensorFlow. В качестве логического завершения задания, вы напишете свой мини-аналог приложения FaceApp. Вам будут даны все  необходимые данные. Предполагается, что для выполнения задания не потребуется GPU.\n",
    "\n",
    "### Ожидаемые результаты\n",
    "\n",
    "В этом задании (как и в оригинальной статье) мы будем проводить эксперименты с лицами людей. При корректном выполнении задания, вы сможете генерировать похожие изображения:\n",
    "\n",
    "![female](./example-results/female.png)\n",
    "![g&m](./example-results/g&m.png)\n",
    "![old](./example-results/old.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 0: изучение статьи\n",
    "\n",
    "Как и при решении любой другой задачи, связанной с обучением глубоких сетей, первым делом стоит разобраться в деталях реализуемого алгоритма. По этой [ссылке](https://arxiv.org/abs/1611.05507) вы найдете статью, которую нужно изучить. После изучения  вы должны иметь возможность ответить себе на следующие вопросы:\n",
    "\n",
    "1. Какая задача решается? Какие данные для этого нужны? Какие требования к модели?\n",
    "2. Каким образом получается векторное представление нашего изображения?\n",
    "3. Как получить вектор сдвига, удовлетворяющий нашим требованиям?\n",
    "4. Как лучше всего подбирать изображения-кандидаты?\n",
    "5. Какой функционал мы минимизируем? Каким образом?\n",
    "6. Как можно улучшить качество результата (убрать артефакты)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install these packages:\n",
    "\n",
    "# conda create -n dfi python=3.5\n",
    "# conda install jupyter\n",
    "# source activate dfi\n",
    "# jupyter notebook\n",
    "\n",
    "import tensorflow as tf      # conda install tensorflow[-gpu]\n",
    "from PIL import Image        # conda install pillow\n",
    "from tqdm import tqdm        # conda install tqdm\n",
    "import numpy as np           # conda install numpy\n",
    "import cv2                   # conda install -c menpo opencv\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt # conda install matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 1: загрузка данных\n",
    "\n",
    "### Описание базы\n",
    "\n",
    "Специально для этого задания была создана база, состоящая более чем из 200K изображений лиц, имеющих разметку по возрасту, полу, расе, типу растительности на лице и типу надетых очков. Разметка была получена в автоматическом режиме при помощи имеющейся модели. Хоть эта база и содержит ошибки, используя её, можно получить очень хороший результат.\n",
    "\n",
    "Все изображения лиц были выровненые с использованием стороннего инструмента. Кропы лиц были получены при помощи детектора лиц OpenCV.\n",
    "\n",
    "### Загрузка базы\n",
    "\n",
    "Для выполнения задания вам потребуется загрузить этот датасет себе на компьютер. Ссылки для скачивания вам должен был предоставить тот, кто выдал вам это задание. Скачав архив себе на комьютер, разархивируйте его. Данный имеют следующий формат: \n",
    " * директория `images` содержит JPEG изображениями лиц (уже подготовленные)\n",
    " * текстовый файл `gt.csv` содержит разметкой для этих изображений\n",
    " * директория `params` хранит вспомогательные файлы\n",
    "\n",
    "Загрузим разметку. В коде ниже не забудьте указать вернуть путь к директории базы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(directory):\n",
    "    header = []\n",
    "    labels = []\n",
    "    filepaths = []\n",
    "    \n",
    "    with open(os.path.join(directory, 'gt.csv')) as f:\n",
    "        header = f.readline().strip().split()[1:]\n",
    "        header = [h.lower() for h in header]\n",
    "        for line in f:\n",
    "            fields = line.strip().split()\n",
    "            path = os.path.join(directory, fields[0])\n",
    "            filepaths.append(path)\n",
    "            labels.append(fields[1:])\n",
    "\n",
    "    return (\n",
    "        header,\n",
    "        np.array(labels),\n",
    "        np.array(filepaths)\n",
    "    )\n",
    "\n",
    "# Set path to dataset on your computer\n",
    "DATASET_DIRECTORY_PATH = './data'  \n",
    "header, labels, filepaths = load_dataset(DATASET_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обзор атрибутов\n",
    "\n",
    "Для понимания, какие атрибуты вам доступны и в каком количестве, посмотрите на круговые диаграммы ниже. Они описывают  распределение значений таких атрибутов как **раса**, **пол**, **тип очков** и **тип растительности на лице**, а так же посмотрите на столбцовую диаграмму, описывающую распределение людей **по возрастам**. Как видно, некоторые значения атрибутов встречаются в базе редко, поэтому учитывайте это при работе с алгоритмом.\n",
    "\n",
    "![attributes](./stats/attributes.png)\n",
    "![ages](./stats/ages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично. Данные готовы. Для удобной работы с ними реализуем служебные функции. Функция `get_mask` даст нам возможность удобно работать с подвыборками нашей базы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['race', 'gender', 'age', 'glasses', 'facial_hair']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['caucasian', 'female', '26', 'no', 'shaved'],\n",
       "       ['caucasian', 'female', '27', 'no', 'shaved'],\n",
       "       ['caucasian', 'male', '41', 'no', 'g&m'],\n",
       "       ['caucasian', 'female', '34', 'no', 'shaved'],\n",
       "       ['caucasian', 'female', '47', 'no', 'shaved'],\n",
       "       ['caucasian', 'male', '47', 'no', 'shaved'],\n",
       "       ['caucasian', 'female', '37', 'no', 'shaved'],\n",
       "       ['asian', 'female', '27', 'no', 'shaved'],\n",
       "       ['caucasian', 'female', '20', 'no', 'shaved'],\n",
       "       ['caucasian', 'female', '28', 'no', 'shaved']], dtype='<U11')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./data/images/7a2ed2cec5445c1d35ac25bed1ac3b0c.jpg',\n",
       "       './data/images/85b838c72094873c51528631e3f427d7.jpg',\n",
       "       './data/images/be2024cd2dacd53a8eeb0cef31b9001b.jpg',\n",
       "       './data/images/e4cd95bf8a3df0959560c008b728a6c0.jpg',\n",
       "       './data/images/f240fb2e4a0fb35109a382aa162edb51.jpg',\n",
       "       './data/images/fa6355b1476a6a121451fc7317b03276.jpg',\n",
       "       './data/images/eda28eef1a0ea9f61b9822f86394b6b9.jpg',\n",
       "       './data/images/e4fb0cc69408811e6acb6c124f6ee798.jpg',\n",
       "       './data/images/8ee9907b5a63ee4b82951ce7533ec62a.jpg',\n",
       "       './data/images/cd0e7bc7d83c1c33a0a0a05364657a70.jpg'], dtype='<U50')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mask(**kwargs):\n",
    "    mask = np.ones(len(filepaths), dtype=np.bool)\n",
    "    for h in kwargs:\n",
    "        if isinstance(kwargs[h], list):\n",
    "            mask &= np.in1d(labels[:, header.index(h)], kwargs[h])\n",
    "        else:\n",
    "            mask &= labels[:, header.index(h)] == kwargs[h]\n",
    "    return mask\n",
    "\n",
    "def load_image(path, size=(128, 128)):\n",
    "    img = Image.open(path)\n",
    "    img = img.convert(\"RGB\")\n",
    "    img = img.resize(size, Image.BICUBIC)\n",
    "    return img\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    return np.float32(np.array(image) / 255.0)[np.newaxis, :, :, :]\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    return Image.fromarray(np.uint8(tensor[0] * 255.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: реализация модели\n",
    "\n",
    "Для вашего удобства в коде ниже приведено почти полное описание модели, с которой мы будем работать. Это VGG-16, предобученная на ImageNet. Несмотря на то, что на данный момент более современные архитектуры во многих задачах превосходят её по скорости и точности (например, ResNet), VGG до сих пор может применяться в задачах, где нужно извлекать интепретируемые признаки, например, для переноса стиля.\n",
    "\n",
    "### Задача\n",
    "\n",
    "Функция построения архитектуры и загрузки весов модели уже реализована (не забудьте указать путь к файлу с весами модели `weights.npy`, прилагающийся к архиву). Пока эта функция не возращает тензор, соответствующего вектор-признаку, который нужен нашему алгоритму. **Напишите необходимый код, чтобы функция его вернула.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_VGG_WEIGHTS_FILE = './data/params/weights.npy'\n",
    "weights_init = np.load(PATH_TO_VGG_WEIGHTS_FILE).item()\n",
    "channel_mean = np.array([103.939, 116.779, 123.68])\n",
    "\n",
    "\n",
    "def phi(z):\n",
    "    image = z * 255.0\n",
    "    red, green, blue = tf.split(image, 3, axis=3)\n",
    "    data = tf.concat([\n",
    "        blue - channel_mean[0],\n",
    "        green - channel_mean[1],\n",
    "        red - channel_mean[2],\n",
    "    ], 3, name='image_preprocessed')\n",
    "    \n",
    "    def max_pool(x, name):\n",
    "        return tf.nn.max_pool(\n",
    "            x, name=name,\n",
    "            ksize=[1, 2, 2, 1], \n",
    "            strides=[1, 2, 2, 1], \n",
    "            padding='SAME')\n",
    "\n",
    "    def conv_layer(x, name):\n",
    "        weights = tf.Variable(weights_init[name][0], name=\"weights\", trainable=False)\n",
    "        biases = tf.Variable(weights_init[name][1], name=\"biases\", trainable=False)\n",
    "        x = tf.nn.conv2d(x, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, biases)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "    conv1_1 = conv_layer(data, \"conv1_1\")\n",
    "    conv1_2 = conv_layer(conv1_1, \"conv1_2\")\n",
    "    pool1 = max_pool(conv1_2, 'pool1') \n",
    "    # -> 112x112x64\n",
    "\n",
    "    conv2_1 = conv_layer(pool1, \"conv2_1\")\n",
    "    conv2_2 = conv_layer(conv2_1, \"conv2_2\")\n",
    "    pool2 = max_pool(conv2_2, 'pool2') \n",
    "    # -> 56x56x128\n",
    "\n",
    "    conv3_1 = conv_layer(pool2, \"conv3_1\")\n",
    "    conv3_2 = conv_layer(conv3_1, \"conv3_2\")\n",
    "    conv3_3 = conv_layer(conv3_2, \"conv3_3\")\n",
    "    conv3_4 = conv_layer(conv3_3, \"conv3_4\")\n",
    "    pool3 = max_pool(conv3_4, 'pool3') \n",
    "    # -> 28x28x256\n",
    "\n",
    "    conv4_1 = conv_layer(pool3, \"conv4_1\")\n",
    "    conv4_2 = conv_layer(conv4_1, \"conv4_2\")\n",
    "    conv4_3 = conv_layer(conv4_2, \"conv4_3\")\n",
    "    conv4_4 = conv_layer(conv4_3, \"conv4_4\")\n",
    "    pool4 = max_pool(conv4_4, 'pool4') \n",
    "    # -> 14x14x512\n",
    "\n",
    "    conv5_1 = conv_layer(pool4, \"conv5_1\")\n",
    "    conv5_2 = conv_layer(conv5_1, \"conv5_2\")\n",
    "    conv5_3 = conv_layer(conv5_2, \"conv5_3\")\n",
    "    conv5_4 = conv_layer(conv5_3, \"conv5_4\")\n",
    "    pool5 = max_pool(conv5_4, 'pool5') \n",
    "    # -> 7x7x512\n",
    "\n",
    "    ###############################\n",
    "    # ...\n",
    "    ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: реализация функции потерь\n",
    "\n",
    "Далее нам потребуется реализовать функцию потерь для нашей задачи. Как вам должно быть известно, эта функция зависит от самого изображения, от его вектор-признака и от целевого-вектор признака.\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Реализуйте функцию ниже, чтобы она возращала тензор нулевого порядка (скаляр), соответствующий значению функции потерь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dfi_loss(z, phi_z, phi_x_shifted):\n",
    "    ######################################################\n",
    "    # ...\n",
    "    ######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: задание оптимизатора\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Реализуйте функцию, которая получает на вход значение функционала потерь и изображение, по которому мы будем производить оптимизацию.** \n",
    "\n",
    "На выходе эта функция должна возращать оператор, вызывая который происходит один шаг градиентного спуска (разберитесь, как работать с оптимизаторами в Tensorflow). Политику learning rate реализуйте на ваше усмотрение.\n",
    "\n",
    "Может получиться так, что в TensorFlow не будет нужного оптимизатора. В этом случае выберите оптимизатор, опираясь на интуицию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_optimizer(loss, z):\n",
    "    #############################################################################\n",
    "    # ...\n",
    "    #############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: описание переменных и операций над ними\n",
    "\n",
    "До этого мы реализовали, функции, которые на вход получают тензоры и на выходе выдают тензоры. Теперь нам нужно описать непосредственно входные тензоры. Пусть \n",
    "* тензор `z` представляет собой изображение, которое мы хотим оптимизировать, \n",
    "* тензор `phi_z` соответствует вектор-признаку, извлеченного из данного изображения,\n",
    "* тензор `phi_z_shifted` -- значение `phi_z`, которое мы стримимся получить, оптимизируя по `z`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = tf.Variable(\n",
    "    np.zeros((1, 128, 128, 3)), \n",
    "    dtype=tf.float32, \n",
    "    trainable=True)\n",
    "phi_z = phi(z)\n",
    "phi_x_shifted = tf.Variable(\n",
    "    np.zeros(phi_z.shape), \n",
    "    dtype=tf.float32, \n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = dfi_loss(z, phi_z, phi_x_shifted)\n",
    "train_op = setup_optimizer(loss, z)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем вспомогательные функции, упрощающие извлечение вектор-признаков из изображений. Функции ниже принимают опциональные аргементы mean и components. Они соответствуют одноименным парамтрам алгоритма PCA, который мы будем использовать для снижения размерности признакового пространства, чтобы мы могли искать по большой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "\n",
    "def extract(image, mean=None, components=None):\n",
    "    vector = sess.run(phi_z, {\n",
    "        z: image_to_tensor(image)\n",
    "    })\n",
    "    if mean is not None:\n",
    "        vector -= mean[0]\n",
    "    if components is not None:\n",
    "        vector = np.dot(vector, components)[0]\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_multiple(paths, mean=None, components=None):\n",
    "    dims = phi_z.shape[-1] if mean is None else components.shape[1]\n",
    "    array = np.zeros((len(paths), dims), dtype=np.float32)\n",
    "    for i in tqdm(range(len(paths))):\n",
    "        image = load_image(paths[i], size=IMG_SIZE)\n",
    "        array[i, ...] = extract(image, mean=mean, components=components)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 6: снижение размерности пространства\n",
    "\n",
    "Как вы должны были заметить, вектор-признак имеет очень большую размерность, что существенно усложняет работу даже с не очень большим количеством векторов (не говоря уже о размерах, сопоставих с нашей выборкой). В статье авторы не описывали этот подход, но как оказалось, он вполне неплохо работает на практике.\n",
    "\n",
    "Суть заключается в том, что чтобы производить поиск ближайших соседей в пространстве меньшей размерности.\n",
    "\n",
    "Поскольку извлечение этих признаков и построение отображение в пространство меньшей размерности методом PCA ресурсоемкая задача, это было проделано за вас. От вас лишь требуется указать ниже пути на numpy-файлы, хранящие параметры преобразования, а так же уже предпосчитанные отображенные вектор-признаки. Очевидно, что мы не сможем их использовать в процессе оптимизации, но они могут существенно упростить процесс подбора кондидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_NPY_FILEPATH = './data/params/_mean.npy'\n",
    "COMPONENTS_NPY_FILEPATH = './data/params/_components.npy'\n",
    "VECTORS_NPY_FILEPATH = './data/params/vectors.npy'\n",
    "\n",
    "if os.path.exists(MEAN_NPY_FILEPATH) and os.path.exists(COMPONENTS_NPY_FILEPATH):\n",
    "    mean, components = np.load(MEAN_NPY_FILEPATH), np.load(COMPONENTS_NPY_FILEPATH)\n",
    "else:\n",
    "    N_SAMPLES = 5000\n",
    "    paths = np.random.permutation(filepaths)[:N_SAMPLES]\n",
    "    array = extract_multiple(paths)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=256, copy=False, whiten=False)\n",
    "    pca.fit(array)\n",
    "    mean = pca.mean_.reshape(1, -1)\n",
    "    components = pca.components_.transpose()\n",
    "    np.save(MEAN_NPY_FILEPATH, mean)\n",
    "    np.save(COMPONENTS_NPY_FILEPATH, components)\n",
    "    \n",
    "\n",
    "if os.path.exists(VECTORS_NPY_FILEPATH):\n",
    "    database = np.load(VECTORS_NPY_FILEPATH)\n",
    "else:\n",
    "    database = extract_multiple(filepaths, mean=mean, components=components)\n",
    "    np.save(VECTORS_NPY_FILEPATH, database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 7: вычисление вектора смещения\n",
    "\n",
    "Назначим функцию `best_samples_ids` ответственной за выбор \"наилучших\" объектов из данной подвыборки для последующего использования этих \"лучших\" объектов в нашем алгоритме.\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Реализуйте функцию `best_samples_ids`.** На вход она принимает вектор-запрос и матрицу векторов, а возращает индексы строк матрицы, соответствующие лучшим векторам для данного запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_ids(query, vectors):\n",
    "    ####################################################\n",
    "    # return np.random.permutation(len(vectors))[:k]\n",
    "    ####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_paths(query, **kwargs):\n",
    "    set_mask = get_mask(**kwargs)\n",
    "    ids = get_best_ids(query, database[set_mask])\n",
    "    set_paths = filepaths[set_mask][ids]\n",
    "    return set_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача\n",
    "\n",
    "**Реализуйте вычисление вектора-смещения.** Функция `get_shift` принимает на вход изображение, а так же два словаря с исходными и целевыми значениями атрибутов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shift(image, source_params, target_params):\n",
    "    query = extract(image, mean=mean, components=components)\n",
    "    source_set_paths = get_best_paths(query, **source_params)\n",
    "    target_set_paths = get_best_paths(query, **target_params)\n",
    "    \n",
    "    source_vectors = extract_multiple(source_set_paths)\n",
    "    target_vectors = extract_multiple(target_set_paths)\n",
    "    ##################################################################\n",
    "    # ...\n",
    "    ##################################################################\n",
    "    return shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 8: оптимизация\n",
    "\n",
    "На данный момент у нас всё готово, чтобы сделать главное.\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Реализуйте функцию `inverse`.** В ней должен происходить некоторый оптимизационный процесс для данного изобржаения и вектора-смещения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse(image, shift):\n",
    "    ###############################################################\n",
    "    # ...\n",
    "    ###############################################################\n",
    "    \n",
    "    return z.eval(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пришло время опробовать реализованную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = load_image('./data/images/f04119339fa749ac34f0a27ed9f6725f.jpg')\n",
    "shift = get_shift(\n",
    "    image, \n",
    "    source_params=dict(\n",
    "        age=list(map(str, range(18, 40))),\n",
    "        gender='male',\n",
    "        race='caucasian',\n",
    "        glasses='no',\n",
    "        facial_hair='shaved'\n",
    "    ), \n",
    "    target_params=dict(\n",
    "        age=list(map(str, range(18, 40))),\n",
    "        gender='male',\n",
    "        race='asian',\n",
    "        glasses='no',\n",
    "        facial_hair='shaved'\n",
    "    )\n",
    ")\n",
    "\n",
    "tensor_to_image(inverse(image, shift * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 9: улучшение результата\n",
    "\n",
    "Как вы могли заметить, в результате оптимизации полученные изображения, хоть и приобретают желаемые черты, становятся несколько неестественными (изменяется цвет, добавляется сильно размытие). То есть становятся сильно отличающимися от исходных изображений. \n",
    "\n",
    "В статье рассматривается эта проблема, а так же приводится её решение.\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Реализуйте функцию inverse_improved, которая позволяет бороться с появление подобных нежелательных эффектов.** Внутри этой функции вы можете вызывать уже имеющуюся функцию `inverse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_improved(image, shift):\n",
    "    ##########################################\n",
    "    # ...\n",
    "    ##########################################\n",
    "    return _fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensor_to_image(inverse_improved(image, shift * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 10: движение в пространстве\n",
    "\n",
    "### Задача\n",
    "\n",
    "Попробуйте произвести интерполяцию сразу по двум направлениям. То есть **варьируя параметр alpha для каждого из направлений, постройте двумерный массив изображений**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################################\n",
    "# ...\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 11: работа с произвольными изображениями\n",
    "\n",
    "Как вы могли заметить, до этого момента мы эксперементировали только с изображениями из предоставленной базы. Эти изображения унифицированы (то есть приведены к общему виду: одинаковый кроп, выравнивание). А хотелось бы модифицировать изображения лиц не только из этой базы. Для этого нужно научиться извлекать изображение лица из фотографии.\n",
    "\n",
    "Воспользуемся наиболее простым способом, а именно: реализацией каскадов Хаара из библиотеки OpenCV. Поскольку мы сможем обнаруживать лица, но сможем их выравнивать, желательно, чтобы лица уже находились в выровненном положении.\n",
    "\n",
    "### Задача\n",
    "\n",
    "**Воспользуясь документацией, разберитесь, как нужно работать с cv2.CascadeClassifier.** Параметры каскада прилагаются к архиву."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HAAR_CASCADE_FILEPATH = './data/params/haarcascade_frontalface_default.xml'\n",
    "cascade = cv2.CascadeClassifier(HAAR_CASCADE_FILEPATH)\n",
    "\n",
    "def get_face_bbox(image):\n",
    "    ##################################\n",
    "    # ...\n",
    "    ##################################\n",
    "    return x, y, x + w, y + h\n",
    "\n",
    "def get_face_image(image):\n",
    "    bbox = get_face_bbox(image)\n",
    "    image = image.crop(bbox)\n",
    "    image = image.resize((128, 128), Image.BICUBIC)\n",
    "    image = image.convert(\"RGB\")\n",
    "    return image, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача\n",
    "\n",
    "Реализуйте функцию вставки лица обратно в изображение. Обратите внимание, что если просто вставлять картинку обратно в изображение, результат появляется очень неестествненый: на границах проявляется явное нессответствие цветов пикселей. Придумайте несложное решение для обхода этой проблемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paste_face_back(image, face_image, bbox):\n",
    "    ################################################################\n",
    "    # ...\n",
    "    ################################################################\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финальный результат\n",
    "\n",
    "Продемонстрируйте работу вашего алгоритма на изображении любого человека."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shift = get_shift(\n",
    "    image, \n",
    "    source_params=dict(\n",
    "        age=list(map(str, range(18, 25))),\n",
    "        gender='male',\n",
    "        race='caucasian',\n",
    "        glasses='no',\n",
    "        facial_hair='shaved'\n",
    "    ), \n",
    "    target_params=dict(\n",
    "        age=list(map(str, range(18, 25))),\n",
    "        gender='female',\n",
    "        race='black',\n",
    "        glasses='no',\n",
    "        facial_hair='shaved'\n",
    "    )\n",
    ")\n",
    "source_image = Image.open('celebrity.jpg')\n",
    "source_image = source_image.resize(\n",
    "    (source_image.width // 4, source_image.height // 4), Image.BICUBIC)\n",
    "face_image, bbox = get_face_image(source_image)\n",
    "face_image = tensor_to_image(inverse_improved(face_image, shift))\n",
    "target_image = paste_face_back(source_image, face_image, bbox)\n",
    "result = Image.new(\"RGB\", (source_image.width * 2, source_image.height))\n",
    "result.paste(source_image, (0, 0))\n",
    "result.paste(target_image, (source_image.width, 0))\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
